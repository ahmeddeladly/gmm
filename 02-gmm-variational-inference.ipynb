{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda67945",
   "metadata": {},
   "source": [
    "# Variational Bayesian in Gaussian Mixture Model (GMM)\n",
    "\n",
    "In **variational Bayesian (variational Bayes, variational inference)**, the algorithm is based on lower bound (called, variational lower bound, evidence lower bound, or ELBO) and KL-divergence, and we aim to get the optimal lower bound by minimizing KL-divergence (i.e, minimizing the difference between distributions). Same as in [EM algorithms](./01-gmm-em-algorithm.ipynb), we also repeat cyclic optimization to get the optimal distribution.<br>\n",
    "In this method, however, we apply Bayesian approach, in which we don't just infer the best values of parameters, but we instead infer the distribution of parameters.\n",
    "\n",
    "Same as in [EM algorithms](./01-gmm-em-algorithm.ipynb), we consider $p(X)$ when $ X=\\{\\mathbf{x}_0, \\ldots, \\mathbf{x}_{N-1}\\} $ is given.<br>\n",
    "In variational Bayesian, we consider the distribution of parameters $q(\\Theta)$, and approximate $q(\\Theta)$ by $q(\\Theta)=\\prod_{i=0}^{M-1} q_i(\\Theta_i)$, in which each $q_i(\\Theta_i)$ is a sub group of parameters. By Bayesian approach, each $q_i(\\Theta_i)$ will then be represented with the corresponding conjugate prior.\n",
    "\n",
    "> Note : The conjugate prior is the distribution which has same expression between prior distribution and posterior distribution in Bayesian. For instance, the conjugate prior for parameters in Bernoulli distribution is known to be the beta distribution. (In exponential family, there always exists the conjugate prior.)\n",
    "\n",
    "By applying above optimization (with evidence lower bound and KL-divergence), it's known that we can obtain the optimal $q_j^*(\\Theta_j)$ as follows, for each $j$. (See Chapter 10 in \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\" for reason.)\n",
    "\n",
    "$ \\ln q_j^*(\\Theta_j) = \\mathbb{E}_{i \\neq j}[\\ln p(X, \\Theta)] + const \\;\\;\\;\\;\\; (1) $\n",
    "\n",
    "where $ \\mathbb{E}_{i \\neq j}[\\cdot] $ is an expectation with respect to $q$ distribution, and then $ \\mathbb{E}_{i \\neq j}[\\ln p(X, \\Theta)]=\\int \\ln p(X, \\Theta) \\prod_{i \\neq j} q_i d\\Theta_i $\n",
    "\n",
    "The equation (1) implies that you can obtain the expectation in some factor using the expectations of other factors.<br>\n",
    "By setting initial expectations in (1), we can then update each expectation and repeat this cycle until we can get the optimal results.\n",
    "\n",
    "> Note : Here I skip the explanation, but you can also evaluate the convergence during cyclic repeats by monitoring evidence lower bound (ELBO).\n",
    "\n",
    "Now let's apply this method in Gaussian Mixture Models (GMM).\n",
    "\n",
    "Gaussian Mixture Model (GMM) $ p(\\mathbf{x}) = \\sum^{K-1}_{k=0} \\pi_k \\mathcal{N}(\\mathbf{x}|\\mu_k, \\Lambda_k^{-1}) $ is decomposed by categorical distribution $ p(\\mathbf{z}) $, in which $ p(z_k=1)=\\pi_k $ (where $\\sum_k \\pi_k = 1.0$), and the corresponding Gaussian distribution (normal distribution) with mean $ \\mu=(\\mu_0, \\mu_1, \\ldots, \\mu_{K-1}) $ and precision $ \\Lambda=(\\Lambda_0, \\Lambda_1, \\ldots, \\Lambda_{K-1}) $.<br>\n",
    "This will be represented by the following graphical model.\n",
    "\n",
    "![GMM Graphical Model](images/gmm_graphical_model02.png)\n",
    "\n",
    "> Note : Here I denote by using precision matrices $\\Lambda$ rather than covariance matrices $\\Sigma$ used in EM algorithm.\n",
    "\n",
    "In this graphical model, the distribution of $ X $ and $ Z $ will be given by using the condition $\\pi$ as follows.\n",
    "\n",
    "$ p(Z|\\pi) = \\prod_{n=0}^{N-1} \\prod_{k=0}^{K-1} \\pi_k^{z_{nk}} \\;\\;\\;\\;\\; (2) $\n",
    "\n",
    "$p(X|Z, \\mu, \\Lambda) = \\prod_{n=0}^{N-1} \\prod_{k=0}^{K-1} \\mathcal{N}(x_n|\\mu_k, \\Lambda_k^{-1})^{z_{nk}} \\;\\;\\;\\;\\; (3) $\n",
    "\n",
    "In this graphical model, $\\{ \\pi, \\mu, \\Lambda \\}$ are parameters and we'll represent these prior distributions in Bayesian.\n",
    "\n",
    "First, $\\pi$ is multinomial distribution, and it's known that the conjugate prior for this distribution is Dirichlet distribution $Dir(\\cdot | \\alpha)$. In variational Bayesian method in GMM, we set initial value $(\\alpha_0, \\ldots, \\alpha_0)$ for $\\alpha$ in this Dirichlet distribution.\n",
    "\n",
    "$ p(\\pi) = Dir(\\pi|(\\alpha_0, \\ldots, \\alpha_0)) = \\frac{\\Gamma(K \\cdot \\alpha_0)}{\\Gamma(\\alpha_0)\\cdots\\Gamma(\\alpha_0)} \\prod_{k=0}^{K-1} \\pi_k^{\\alpha_0-1} \\;\\;\\;\\;\\; (4) $\n",
    "\n",
    "where $\\Gamma(\\cdot)$ is Gamma function.\n",
    "\n",
    "Next, we consider each Gaussian distribution components. It's known that the conjugate prior for mean and precision in multivariate Gaussian distribution is given by the following equation with Wishart distribution (which is called Gaussian-Wishart prior). Here we set initial values $\\mathbf{m}_0, \\beta_0, \\nu_0$ and initial matrix $W_0$.\n",
    "\n",
    "$ p(\\mu, \\Lambda) = p(\\mu|\\Lambda)p(\\Lambda) = \\prod_{k=0}^{K-1} \\left\\{ \\mathcal{N}(\\mu_k|\\mathbf{m}_0,(\\beta_0 \\Lambda_k)^{-1}) \\cdot \\mathcal{W}(\\Lambda_k|W_0,\\nu_0) \\right\\} \\;\\;\\;\\;\\; (5) $\n",
    "\n",
    "where $\\mathcal{W}(\\cdot)$ is Wishart distribution.\n",
    "\n",
    "As you can see above, $\\mu$ depends on $\\Lambda$ in conjugate prior and the graphical model will then be represented as follows.\n",
    "\n",
    "![GMM Graphical Model](images/gmm_graphical_model03.png)\n",
    "\n",
    "In variational Bayesian in Gaussian Mixture Model, we apply the following approximation.\n",
    "\n",
    "$q(Z, \\pi, \\mu, \\Lambda) = q(Z) q(\\pi, \\mu, \\Lambda)$\n",
    "\n",
    "As a result, the latter part in RHS (right-hand side) will be decomposed as follows.\n",
    "\n",
    "$q(Z, \\pi, \\mu, \\Lambda)$\n",
    "\n",
    "$= q(Z) q(\\pi, \\mu, \\Lambda)$\n",
    "\n",
    "$= q(Z) q(\\pi) q(\\mu, \\Lambda)$\n",
    "\n",
    "$= q(Z) q(\\pi) \\prod_{k=0}^{K-1} q(\\mu_k, \\Lambda_k)$\n",
    "\n",
    "Using the equation (1) and above conjugate prior, we can get the expression of optimal $q^*(Z)$, $q^*(\\pi)$, and $q^*(\\mu_k, \\Lambda_k)$.\n",
    "\n",
    "For instance, $q^*(Z)$ will be represented by :\n",
    "\n",
    "$\\ln q^*(Z)=\\mathbb{E}_{\\pi,\\mu,\\Lambda}[\\ln p(X, Z, \\pi, \\mu, \\Lambda)] + const$\n",
    "\n",
    "Because $p(X, Z, \\pi, \\mu, \\Lambda)=p(X|Z, \\mu, \\Lambda)p(Z|\\pi)p(\\pi)p(\\mu,\\Lambda)$ and only $p(X|Z, \\mu, \\Lambda)p(Z|\\pi)$ depends on $Z$, it will then be :\n",
    "\n",
    "$\\ln q^*(Z)=\\mathbb{E}_{\\mu,\\Lambda}[\\ln p(X|Z, \\mu, \\Lambda)]+\\mathbb{E}_{\\pi}[\\ln p(Z|\\pi)] + const$\n",
    "\n",
    "By applying (2) and (3) in above equation, you will obtain the following equation. (I note that the 4th term $\\mathbb{E}_{\\mu_k, \\Lambda_k}[(\\mathbf{x}_n - \\mu_k)^T \\Lambda_k (\\mathbf{x}_n - \\mu_k)]$ only depends on the input's index $n$.)\n",
    "\n",
    "$\\ln q^*(Z)=\\sum_{n=0}^{N-1} \\sum_{k=0}^{K-1} z_{nk} \\rho_{nk} + const$\n",
    "\n",
    "where $ \\rho_{nk}=\\mathbb{E}_{\\pi_k}[\\ln \\pi_k] + \\frac{1}{2} \\mathbb{E}_{\\Lambda_k}[\\ln |\\Lambda_k|] - \\frac{D}{2} \\ln(2\\pi) - \\frac{1}{2} \\mathbb{E}_{\\mu_k, \\Lambda_k}[(\\mathbf{x}_n - \\mu_k)^T \\Lambda_k (\\mathbf{x}_n - \\mu_k)] $ and $D$ is the number of dimension of input $\\mathbf{x}$.\n",
    "\n",
    "By normalizing the results, we will then obtain new (optimal) $q^*(Z)$ as follows.\n",
    "\n",
    "$ q^*(Z) = \\prod_{n=0}^{N-1} \\prod_{k=0}^{K-1} r_{nk}^{z_{nk}} \\;\\;\\;\\;\\; (6) $\n",
    "\n",
    "where $r_{nk}=\\frac{e^{\\rho_{nk}}}{\\sum_{j=0}^{K-1} e^{\\rho_{nj}}}$\n",
    "\n",
    "Here I skip detailed descriptions, but you can also obtain $q^*(\\pi)$ and $q^*(\\mu_k, \\Lambda_k)$ by applying computation (transformation) as follows.<br>\n",
    "(See Chapter 10 in \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\".)\n",
    "\n",
    "$ q^*(\\pi)=Dir(\\pi|\\alpha) \\;\\;\\;\\;\\; (7) $\n",
    "\n",
    "where $\\alpha = (\\alpha_0 + N_0,\\alpha_0 + N_1, \\ldots, \\alpha_0 + N_{K-1})$ and $N_k=\\sum_{n=0}^{N-1} r_{nk}$\n",
    "\n",
    "$ q^*(\\mu_k, \\Lambda_k) = \\mathcal{N}(\\mu_k|\\mathbf{m}_k, (\\beta_k \\Lambda_k)^{-1}) \\mathcal{W}(\\Lambda_k|W_k,\\nu_k) \\;\\;\\;\\;\\; (8) $\n",
    "\n",
    "where\n",
    "\n",
    "$\\beta_k = \\beta_0 + N_k$\n",
    "\n",
    "$\\mathbf{m}_k = \\frac{1}{\\beta_k} (\\beta_0 \\mathbf{m}_0 + N_k \\bar{\\mathbf{x}_k})$\n",
    "\n",
    "$W_k^{-1} = W_0^{-1} + \\sum_{n=0}^{N-1} \\left\\{ r_{nk} ( \\mathbf{x}_n - \\bar{\\mathbf{x}_k} ) ( \\mathbf{x}_n - \\bar{\\mathbf{x}_k} )^T \\right\\} + \\frac{\\beta_0 N_k}{\\beta_0 + N_k}(\\bar{\\mathbf{x}_k} - \\mathbf{m}_0) (\\bar{\\mathbf{x}_k}-\\mathbf{m}_0)^T$\n",
    "\n",
    "$\\nu_k = \\nu_0 + N_k$\n",
    "\n",
    "and $\\bar{\\mathbf{x}_k} = \\frac{1}{N_k} \\sum_{n=0}^{N-1} r_{nk} \\mathbf{x}_n$\n",
    "\n",
    "In variational Bayesian, we repeat the calculation of $r_{nk}$ and the updates of (6), (7), (8) to get the optimal parameters.\n",
    "\n",
    "The obtained result (prediction function) will be given with the marginal distribution by parameters, and later you will find that it leads to well-known Student's t-distribution.<br>\n",
    "When $N$ goes to infinity, the solution of variational Bayesian will go to the solution of EM algorithm.\n",
    "\n",
    "In this example, we assume that $K$ (the number of components in GMM) is known, but even when you set a large number as $K$ in variational Bayesian method, there'll be no overfitting by Bayesian process. (See [here](https://tsmatz.wordpress.com/2017/09/13/overfitting-for-regression-and-deep-learning/) for details about overfitting.)\n",
    "\n",
    "> Note : When you compare models in order to decide $K$ (see [information criterion](https://tsmatz.wordpress.com/2017/09/13/overfitting-for-regression-and-deep-learning/)), please take care for the variation of solutions.<br>\n",
    "> See 10.2.4 in \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\" for details.\n",
    "\n",
    "Now let's start to implement this algorithm and see the result.\n",
    "\n",
    "*back to [Readme](https://github.com/tsmatz/gmm/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c46371",
   "metadata": {},
   "source": [
    "##  Generate sample data (Sampling)\n",
    "\n",
    "I generate sample data (observations) by using the distribution of GMM.<br>\n",
    "In this example, I will generate 1000 samples of $ \\sum_{k=0}^2 \\pi_k \\mathcal{N}(\\mathbf{x}_n|\\mu_k, \\Lambda_k^{-1}) $ (i.e, $ K=3 $) where :\n",
    "\n",
    "$ \\pi_0=0.15, \\pi_1=0.35, \\pi_2=0.5 $\n",
    "\n",
    "$ \\mu_0=(-3.0, 2.0), \\mu_1=(4.0, 2.0), \\mu_2=(3.0, 1.0) $\n",
    "\n",
    "$\\Lambda_0=\\begin{bmatrix} 1.020 & -0.102 \\\\ -0.102 & 0.510 \\end{bmatrix}$ (i.e, $ \\Sigma_0=\\begin{bmatrix} 1.0 & 0.2 \\\\ 0.2 & 2.0 \\end{bmatrix} $)\n",
    "\n",
    "$\\Lambda_1=\\begin{bmatrix} 32.500 & 25.000 \\\\ 25.000 & 20.000 \\end{bmatrix}$ (i.e, $ \\Sigma_1=\\begin{bmatrix} 0.8 & -1.0 \\\\ -1.0 & 1.3 \\end{bmatrix} $)\n",
    "\n",
    "$\\Lambda_2=\\begin{bmatrix} 5.172 & -4.828 \\\\ -4.828 & 5.172 \\end{bmatrix}$ (i.e, $ \\Sigma_2=\\begin{bmatrix} 1.5 & 1.4 \\\\ 1.4 & 1.5 \\end{bmatrix} $)\n",
    "\n",
    "This is the same example in [EM algorithms](./01-gmm-em-algorithm.ipynb), but here I denote with precision $\\Lambda_k$, instead of covariance $\\Sigma_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a118f039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9e69ff0748>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1000)  # For debugging and reproducibility\n",
    "\n",
    "N = 1000\n",
    "D = 2\n",
    "\n",
    "pi1=0.15; pi2=0.35; pi3=0.5\n",
    "\n",
    "mu1=np.array([-3.0, 2.0])\n",
    "mu2=np.array([4.0, 2.0])\n",
    "mu3=np.array([3.0, 1.0])\n",
    "\n",
    "Sigma1 = np.array([\n",
    "    [1.0,0.2],\n",
    "    [0.2,2.0],\n",
    "])\n",
    "Sigma2 = np.array([\n",
    "    [0.8,-1.0],\n",
    "    [-1.0,1.3],\n",
    "])\n",
    "Sigma3 = np.array([\n",
    "    [1.5,1.4],\n",
    "    [1.4,1.5],\n",
    "])\n",
    "\n",
    "Z_seeds = np.random.uniform(0, 1, size=N)\n",
    "X = np.empty((0,2))\n",
    "for z_seed in Z_seeds:\n",
    "    if z_seed < pi1:\n",
    "        x_n = np.random.multivariate_normal(\n",
    "            mean=mu1,\n",
    "            cov=Sigma1,\n",
    "            size=1)\n",
    "    elif z_seed < pi1 + pi2:\n",
    "        x_n = np.random.multivariate_normal(\n",
    "            mean=mu2,\n",
    "            cov=Sigma2,\n",
    "            size=1)\n",
    "    else:\n",
    "        x_n = np.random.multivariate_normal(\n",
    "            mean=mu3,\n",
    "            cov=Sigma3,\n",
    "            size=1)\n",
    "    X = np.vstack((X, x_n))\n",
    "\n",
    "plt.plot(X[:,0], X[:,1], \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb896c56",
   "metadata": {},
   "source": [
    "## Apply variational Bayesian (variational inference) in Python\n",
    "\n",
    "Now let's implement variational Bayesian (variational inference) in Python and estimate the distribution of parameters $q(\\pi)=Dir(\\pi|\\alpha)$ and $q(\\mu_k, \\Lambda_k) = \\mathcal{N}(\\mu_k|\\mathbf{m}_k, (\\beta_k \\Lambda_k)^{-1}) \\mathcal{W}(\\Lambda_k|W_k,\\nu_k) \\;\\; (k=0,1,2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f19bb",
   "metadata": {},
   "source": [
    "### 1. Initialize parameters\n",
    "\n",
    "First, we set initial values $ \\alpha_0, W_0, \\nu_0, m_0, \\beta_0 $ as follows.<br>\n",
    "($W_k$ should be symmetric and positive definite matrix and $\\nu_k$ should be $\\nu_k \\geq D$.)\n",
    "\n",
    "- $\\alpha_0 = 1.5$\n",
    "- $W_0 = \\begin{bmatrix} 3.0 & 1.0 \\\\ 1.0 & 4.0 \\end{bmatrix}$\n",
    "- $\\nu_0 = 2.0$\n",
    "- $\\mathbf{m}_0 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$\n",
    "- $\\beta_0 = 1.5$\n",
    "\n",
    "I note that you should set different values for initial $m_k$ in order to make convergence. (When you set the same values for $k=0,1,2$, it will stuck into the same value.)<br>\n",
    "In this example, I have added Gaussian noise for initial $m_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c07023b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3\n",
    "\n",
    "# Initialize parameters\n",
    "class DistParams:\n",
    "    alpha = np.empty((0,K))\n",
    "    W = np.empty((0,K,2,2))\n",
    "    nu = np.empty((0,K))\n",
    "    m = np.empty((0,K,2))\n",
    "    beta = np.empty((0,K))\n",
    "\n",
    "    def __init__(self, alpha_0, W_0, nu_0, m_0, beta_0):\n",
    "        # Memory initial values\n",
    "        self.alpha_0 = alpha_0\n",
    "        self.W_0 = W_0\n",
    "        self.nu_0 = nu_0\n",
    "        self.m_0 = m_0\n",
    "        self.beta_0 = beta_0\n",
    "\n",
    "        # Memory current values to compute r\n",
    "        # But, in order to make convergence, set different values for initial m\n",
    "        self.alpha = np.array([alpha_0] * K)\n",
    "        self.W = np.array([W_0] * K)\n",
    "        self.nu = np.array([nu_0] * K)\n",
    "        self.m = np.random.multivariate_normal(\n",
    "            m_0,\n",
    "            [[1.0, 0.0],[0.0, 1.0]],\n",
    "            size=K)\n",
    "        self.beta = np.array([beta_0] * K)\n",
    "\n",
    "dist_params = DistParams(\n",
    "    alpha_0=1.5,\n",
    "    W_0=np.array([[1.0, 0.0],[0.0, 1.0]]),\n",
    "    nu_0=2.0,\n",
    "    m_0=np.array([1.0, 1.0]),\n",
    "    beta_0=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22390ebd",
   "metadata": {},
   "source": [
    "### 2. Compute $r_{nk}$\n",
    "\n",
    "Next we get the following $r_{nk}$ :\n",
    "\n",
    "$$ r_{nk}=\\frac{e^{\\rho_{nk}}}{\\sum_{j=0}^{K-1} e^{\\rho_{nj}}} $$\n",
    "\n",
    "where $ \\rho_{nk}=\\mathbb{E}_{\\pi_k}[\\ln \\pi_k] + \\frac{1}{2} \\mathbb{E}_{\\Lambda_k}[\\ln |\\Lambda_k|] - \\frac{D}{2} \\ln(2\\pi) - \\frac{1}{2} \\mathbb{E}_{\\mu_k, \\Lambda_k}[(\\mathbf{x}_n - \\mu_k)^T \\Lambda_k (\\mathbf{x}_n - \\mu_k)] $ and $D=2$\n",
    "\n",
    "In this equation, we can obtain expectations by the following formula. (See \"Appendix B\" in \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\" for expectations in Dirichlet distribution and Wishart distribution.)\n",
    "\n",
    "1. $\\mathbb{E}_{\\pi_k}[\\ln \\pi_k] = \\psi(\\alpha_k) - \\psi(\\sum_k \\alpha_k)$\n",
    "2. $\\mathbb{E}_{\\Lambda_k}[\\ln |\\Lambda_k|] = \\sum_{i=1}^D \\psi \\left( \\frac{\\nu_k + 1 - i}{2} \\right) + D \\ln 2 + \\ln |W_k| $\n",
    "3. $\\mathbb{E}_{\\mu_k, \\Lambda_k}[(\\mathbf{x}_n - \\mu_k)^T \\Lambda_k (\\mathbf{x}_n - \\mu_k)]=\\frac{D}{\\beta_k}+\\nu_k (\\mathbf{x}_n - \\mathbf{m}_k)^T W_k (\\mathbf{x}_n - \\mathbf{m}_k)$\n",
    "\n",
    "where $\\psi(\\cdot)$ is digamma function and $D=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0bfc3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.special import digamma\n",
    "\n",
    "def get_expectation01(alpha):\n",
    "    return digamma(alpha) - digamma(alpha.sum())\n",
    "\n",
    "def get_expectation02(nu, W):\n",
    "    t1 = sum(digamma((nu + 1.0 - (i+1)) / 2.0) for i in range(D))\n",
    "    t2 = D * np.log(2)\n",
    "    t3 = np.log(np.linalg.det(W))\n",
    "    return t1 + t2 + t3\n",
    "\n",
    "def get_expectation03(X, beta, nu, m, W):\n",
    "    def compute_exp03(x_n, beta_k, nu_k, m_k, W_k):\n",
    "        t1 = D / beta_k\n",
    "        t2 = np.dot(np.matmul(x_n - m_k, W_k), x_n - m_k) * nu_k\n",
    "        return t1 + t2\n",
    "    return np.array([[compute_exp03(x, beta[k], nu[k], m[k], W[k]) for k in range(K)] for x in X])\n",
    "\n",
    "def get_rho(arg_dist_params):\n",
    "    exp1 = get_expectation01(arg_dist_params.alpha)\n",
    "    exp2 = get_expectation02(arg_dist_params.nu, arg_dist_params.W)\n",
    "    exp3 = get_expectation03(\n",
    "        X,\n",
    "        arg_dist_params.beta,\n",
    "        arg_dist_params.nu,\n",
    "        arg_dist_params.m,\n",
    "        arg_dist_params.W\n",
    "    )\n",
    "    def compute_rho(exp1_k, exp2_k, exp3_nk):\n",
    "        return exp1_k + exp2_k / 2.0 - D * np.log(2.0 * math.pi) / 2.0 - exp3_nk / 2.0\n",
    "\n",
    "    return np.array([[compute_rho(exp1[k], exp2[k], exp3[n][k]) for k in range(K)] for n in range(N)])\n",
    "\n",
    "def get_r(rho):\n",
    "    rho_exp = np.exp(rho)\n",
    "    return np.array([rho_exp[n]/rho_exp[n].sum() for n in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559ce05",
   "metadata": {},
   "source": [
    "### 3. Compute new $\\alpha_k$\n",
    "\n",
    "Using above $r_{nk}$, we can get new $\\alpha_k$ as follows (which then constructs the Dirichlet distribution $q^*(\\pi)$).\n",
    "\n",
    "$\\alpha_k = \\alpha_0 + N_k$\n",
    "\n",
    "where $N_k=\\sum_{n=0}^{N-1} r_{nk}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f61060ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Nk_arr(r):\n",
    "    return r.sum(axis=0)\n",
    "\n",
    "def get_alpha(alpha_0, Nk_arr):\n",
    "    return alpha_0 + Nk_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e81aae",
   "metadata": {},
   "source": [
    "### 4. Compute new $\\beta_k$\n",
    "\n",
    "New $\\beta_k$ (which consists of new Gaussian-Wishart distribution $q^*(\\mu_k, \\Lambda_k)$) is also obtained as follows.\n",
    "\n",
    "$\\beta = \\beta_0 + N_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5142502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta(beta_0, Nk_arr):\n",
    "    return beta_0 + Nk_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f2d1f1",
   "metadata": {},
   "source": [
    "### 5. Compute new $\\mathbf{m}_k$\n",
    "\n",
    "New $\\mathbf{m}_k$ (which also consists of new Gaussian-Wishart distribution $q^*(\\mu_k, \\Lambda_k)$) is obtained as follows.\n",
    "\n",
    "$\\mathbf{m}_k = \\frac{1}{\\beta_k} (\\beta_0 \\mathbf{m}_0 + N_k \\bar{\\mathbf{x}_k})$\n",
    "\n",
    "where $\\bar{\\mathbf{x}_k} = \\frac{1}{N_k} \\sum_{n=0}^{N-1} r_{nk} \\mathbf{x}_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91940413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xk_arr(X, r, Nk_arr):\n",
    "    def r_by_x(X_arg, r_k):\n",
    "        return np.array([(r_k[n] * X_arg[n]) for n in range(N)])\n",
    "    numer = np.array([r_by_x(X, r[ :,k]).sum(axis=0) for k in range(K)])\n",
    "    return np.array([numer[k]/Nk_arr[k] for k in range(K)])\n",
    "\n",
    "def get_m(m_0, beta_0, beta, Nk_arr, xk_arr):\n",
    "    numer = np.array([(beta_0 * m_0 + Nk_arr[k] * xk_arr[k]) for k in range(K)])\n",
    "    return np.array([numer[k]/beta[k] for k in range(K)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea03b4",
   "metadata": {},
   "source": [
    "### 6. Compute new $W_k$\n",
    "\n",
    "New $W_k$ (which also consists of new Gaussian-Wishart distribution $q^*(\\mu_k, \\Lambda_k)$) is given as follows.\n",
    "\n",
    "$W_k^{-1} = W_0^{-1} + \\sum_{n=0}^{N-1} \\left\\{ r_{nk} ( \\mathbf{x}_n - \\bar{\\mathbf{x}_k} ) ( \\mathbf{x}_n - \\bar{\\mathbf{x}_k} )^T \\right\\} + \\frac{\\beta_0 N_k}{\\beta_0 + N_k}(\\bar{\\mathbf{x}_k} - \\mathbf{m}_0) (\\bar{\\mathbf{x}_k}-\\mathbf{m}_0)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44f25984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_W_inverse(X, W_0, r, beta_0, m_0, Nk_arr, xk_arr):\n",
    "    t1 = np.linalg.inv(W_0)\n",
    "    def t2_n(r_n_k, X_n, xk_arr_k):\n",
    "        v = np.array([X_n - xk_arr_k])\n",
    "        v_t = v.transpose()\n",
    "        return r_n_k * np.matmul(v_t, v)\n",
    "    def t2_k(r_k, X, xk_arr_k):\n",
    "        v = np.array([t2_n(r_k[n], X[n], xk_arr_k) for n in range(N)])\n",
    "        return v.sum(axis=0)\n",
    "    t2 = np.array([t2_k(r[ :,k], X, xk_arr[k]) for k in range(K)])\n",
    "    def t3_k(xk_arr_k, m_0, Nk_arr_k, beta_0):\n",
    "        v = np.array([xk_arr_k - m_0])\n",
    "        v_t = v.transpose()\n",
    "        return np.matmul(v_t, v) * beta_0 * Nk_arr_k / (beta_0 + Nk_arr_k)\n",
    "    t3 = np.array([t3_k(xk_arr[k], m_0, Nk_arr[k], beta_0) for k in range(K)])\n",
    "    return np.array([(t1 + t2[k] + t3[k]) for k in range(K)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c627b",
   "metadata": {},
   "source": [
    "### 7. Compute new $\\nu_k$\n",
    "\n",
    "New $\\nu_k$ (which consists of new Gaussian-Wishart distribution $q^*(\\mu_k, \\Lambda_k)$) is also obtained as follows.\n",
    "\n",
    "$\\nu_k = \\nu_0 + N_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ea2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nu(nu_0, Nk_arr):\n",
    "    return nu_0 + Nk_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9064163",
   "metadata": {},
   "source": [
    "### 8. Put it all together\n",
    "\n",
    "Now we put it all together and run algorithm.\n",
    "\n",
    "In this example, the number of loop is the fixed value. However, by monitoring **variational lower bound** (also called, evidence lower bound or ELBO), you can also evaluate the convergence in variational Bayesian.<br>\n",
    "See 10.2.2 in \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "486e39b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 50 ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for loop in range(50):\n",
    "    print(\"Running iteration {} ...\".format(loop + 1), end=\"\\r\")\n",
    "    # Get r\n",
    "    l_rho = get_rho(dist_params)\n",
    "    l_r = get_r(l_rho)\n",
    "    # Get N_k\n",
    "    l_N_k = get_Nk_arr(l_r)\n",
    "    # Get new alpha\n",
    "    l_alpha = get_alpha(dist_params.alpha_0, l_N_k)\n",
    "    # Get new beta\n",
    "    l_beta = get_beta(dist_params.beta_0, l_N_k)\n",
    "    # Get bar_x_k\n",
    "    l_x_k = get_xk_arr(X, l_r, l_N_k)\n",
    "    # Get new m\n",
    "    l_m = get_m(dist_params.m_0, dist_params.beta_0, l_beta, l_N_k, l_x_k)\n",
    "    # Get new W\n",
    "    l_W_inv = get_W_inverse(X, dist_params.W_0, l_r, dist_params.beta_0, dist_params.m_0, l_N_k, l_x_k)\n",
    "    l_W = np.array([np.linalg.inv(l_W_inv[k]) for k in range(K)])\n",
    "    # Get new nu\n",
    "    l_nu = get_nu(dist_params.nu_0, l_N_k)\n",
    "\n",
    "    # Replace current params for r\n",
    "    dist_params.alpha = l_alpha\n",
    "    dist_params.W = l_W\n",
    "    dist_params.nu = l_nu\n",
    "    dist_params.m = l_m\n",
    "    dist_params.beta = l_beta\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d7345",
   "metadata": {},
   "source": [
    "## Compare result\n",
    "\n",
    "Let's compare and see the estimated results.<br>\n",
    "(In order to simplify the comparison for you, here I have changed the order of components.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dd4613",
   "metadata": {},
   "source": [
    "Our actual value of mean in GMM is $ \\mu_2=(3.0, 1.0), \\mu_1=(4.0, 2.0), \\mu_0=(-3.0, 2.0) $.<br>\n",
    "The mean value in the estimated distribution is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8a84d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.87714931,  0.89628928],\n",
       "       [ 4.03070604,  1.91857709],\n",
       "       [-3.05747205,  1.78215033]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_params.m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c4f1a",
   "metadata": {},
   "source": [
    "Our actual values of precision in GMM is :\n",
    "\n",
    "$\\Lambda_2=\\begin{bmatrix} 5.172 & -4.828 \\\\ -4.828 & 5.172 \\end{bmatrix}$\n",
    "\n",
    "$\\Lambda_1=\\begin{bmatrix} 32.500 & 25.000 \\\\ 25.000 & 20.000 \\end{bmatrix}$\n",
    "\n",
    "$\\Lambda_0=\\begin{bmatrix} 1.020 & -0.102 \\\\ -0.102 & 0.510 \\end{bmatrix}$\n",
    "\n",
    "The precision matrix in the estimated distribution is as follows, since the expected precision $\\mathbb{E}[\\Lambda_k]$ in Wishart distribution $\\mathcal{W}(\\Lambda_k|W_k,\\nu_k)$ is $\\nu_k W_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25bafd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 4.54908732, -4.22230989],\n",
       "        [-4.22230989,  4.55028156]]),\n",
       " array([[8.82139854, 6.67928398],\n",
       "        [6.67928398, 5.85625664]]),\n",
       " array([[ 0.78310269, -0.06362062],\n",
       "        [-0.06362062,  0.51515316]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[dist_params.nu[k] * dist_params.W[k] for k in range(K)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d75dc5a",
   "metadata": {},
   "source": [
    "Our actual probability $\\pi$ in each GMM components is $ \\pi_2=0.5, \\pi_1=0.35, \\pi_0=0.15 $.<br>\n",
    "The estimated probability is as follows, since the expectation $\\mathbb{E}[\\pi_k]$ of component $k$ in Dirichlet distribution $Dir(\\pi|\\alpha)$ is $\\frac{\\alpha_k}{\\sum_k \\alpha_k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca75d49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46797179, 0.35129235, 0.18073586])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "dist_params.alpha / dist_params.alpha.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44706aa7",
   "metadata": {},
   "source": [
    "In this distribution, the predictive density $p(\\hat{x})$ will be given by marginal distribution as follows.\n",
    "\n",
    "$$ p(\\hat{x} | X) = \\sum_{k=0}^{K-1} \\int \\int \\int \\pi_k \\mathcal{N}(\\hat{x}|\\mu_k, \\Lambda_k^{-1}) p(\\pi, \\mu, \\Lambda | X) d\\pi \\; d\\mu \\; d\\Lambda $$\n",
    "\n",
    "This distribution can be approximated by the mixture of Student's t-distributions as follows. (This will meet the well-known fact that marginal distribution of Gaussian distribution in Bayesian is a Student's t-distribution.)<br>\n",
    "When $N \\to \\infty$, the mixture of this Student's t-distributions will go to the mixture of Gaussian ditsirbution (i.e, GMM).\n",
    "\n",
    "$$ p(\\hat{x} | X) = \\frac{1}{\\sum_k \\alpha_k} \\sum_{k=1}^{K-1} \\alpha_k St \\left( \\hat{x} \\; \\middle| \\; \\mathbf{m}_k, \\frac{(\\nu_k + 1 - D) \\beta_k}{(1+\\beta_k)} W_k, \\nu_k + 1 - D \\right) $$\n",
    "\n",
    "> Note : See 10.2.3 in \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\" for details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
